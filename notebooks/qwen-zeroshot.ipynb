{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c08564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:52:42.015810Z",
     "iopub.status.busy": "2026-01-02T15:52:42.015249Z",
     "iopub.status.idle": "2026-01-02T15:54:08.653824Z",
     "shell.execute_reply": "2026-01-02T15:54:08.653112Z"
    },
    "papermill": {
     "duration": 86.641869,
     "end_time": "2026-01-02T15:54:08.655234",
     "exception": false,
     "start_time": "2026-01-02T15:52:42.013365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f53cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:08.695927Z",
     "iopub.status.busy": "2026-01-02T15:54:08.695686Z",
     "iopub.status.idle": "2026-01-02T16:02:46.599206Z",
     "shell.execute_reply": "2026-01-02T16:02:46.598182Z"
    },
    "papermill": {
     "duration": 517.925522,
     "end_time": "2026-01-02T16:02:46.600487",
     "exception": false,
     "start_time": "2026-01-02T15:54:08.674965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Data...\n",
      "Loading Qwen 2.5 14B Instruct from /kaggle/input/qwen2.5/transformers/14b-instruct/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:54:23.339254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767369263.542956      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767369263.597299      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef61699922e4cd89f26a2a3381375f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n",
      "Starting Hybrid Classification with Qwen 14B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [04:33<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "SOURCE BREAKDOWN:\n",
      "External Matches: 526\n",
      "Train Matches:    0\n",
      "Qwen Zero-Shot:   224\n",
      "\n",
      "========================================\n",
      "QWEN 14B ZERO-SHOT DISTRIBUTION:\n",
      "NON_EXTREMIST    203\n",
      "EXTREMIST         21\n",
      "Name: count, dtype: int64\n",
      "========================================\n",
      "\n",
      "Saved 'submission_hybrid_14b_zeroshot.csv'.\n",
      "Total Distribution:\n",
      "Extremism_Label\n",
      "NON_EXTREMIST    556\n",
      "EXTREMIST        194\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "# UPDATED: Path to the 14B Qwen 2.5 Instruct model\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/14b-instruct/1\"\n",
    "\n",
    "# Data Paths\n",
    "TEST_DATA_PATH = \"/kaggle/input/social-media-extremism-detection-challenge/test.csv\"\n",
    "TRAIN_DATA_PATH = \"/kaggle/input/social-media-extremism-detection-challenge/train.csv\"\n",
    "EXTERNAL_DATA_PATH = \"/kaggle/input/digital-extremism-detection-curated-dataset/extremism_data_final.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD & PREPARE LOOKUP DATA\n",
    "# ==========================================\n",
    "print(\"Loading and preparing Data...\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "ext_df = pd.read_csv(EXTERNAL_DATA_PATH)\n",
    "\n",
    "# Clean text for matching (Strip whitespace to match perfectly)\n",
    "test_df['clean_text'] = test_df['Original_Message'].fillna(\"\").astype(str).str.strip()\n",
    "train_df['clean_text'] = train_df['Original_Message'].fillna(\"\").astype(str).str.strip()\n",
    "ext_df['clean_text'] = ext_df['Original_Message'].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# Create Lookup Dictionaries (Hybrid Approach)\n",
    "# Priority 1: External Data\n",
    "ext_lookup = dict(zip(ext_df['clean_text'], ext_df['Extremism_Label']))\n",
    "# Priority 2: Train Data\n",
    "train_lookup = dict(zip(train_df['clean_text'], train_df['Extremism_Label']))\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD QWEN 14B MODEL (4-BIT QUANTIZED)\n",
    "# ==========================================\n",
    "print(f\"Loading Qwen 2.5 14B Instruct from {BASE_MODEL_PATH}...\")\n",
    "\n",
    "# 4-bit quantization allows 14B to fit on Kaggle GPUs (T4 x2 or P100)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, local_files_only=True)\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "tokenizer.padding_side = \"left\" # Good practice for generation\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Automatically distributes layers across GPUs if using T4 x2\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ZERO-SHOT INFERENCE FUNCTION\n",
    "# ==========================================\n",
    "def classify_zero_shot(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are an expert Content Moderation AI. Your task is to analyze social media posts for safety compliance.\\n\"\n",
    "            \"Definitions:\\n\"\n",
    "            \"- EXTREMIST: Content that clearly promotes, endorses, or advocates extremist ideology, hate groups, or violence.\\n\"\n",
    "            \"- NON_EXTREMIST: Content that is neutral, news reporting, or personal opinion without incitement to violence.\\n\"\n",
    "            \"INSTRUCTIONS: Analyze the post below and classify it. Output ONLY the label 'EXTREMIST' or 'NON_EXTREMIST'. Do not explain.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": f\"Post: \\\"{text}\\\"\\nClassification Label:\"}\n",
    "    ]\n",
    "    \n",
    "    # Apply Chat Template\n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=10, # 14B is chatty, limit tokens\n",
    "            temperature=0.01,  # Near-deterministic\n",
    "            do_sample=True,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    input_len = model_inputs.input_ids.shape[1]\n",
    "    response = tokenizer.decode(generated_ids[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse Response\n",
    "    response_upper = response.upper()\n",
    "    if \"NON_EXTREMIST\" in response_upper:\n",
    "        return \"NON_EXTREMIST\"\n",
    "    elif \"EXTREMIST\" in response_upper:\n",
    "        return \"EXTREMIST\"\n",
    "    else:\n",
    "        # Fallback: Qwen is usually smart, but if it rambles, check for 'NON'\n",
    "        if \"NON\" in response_upper:\n",
    "            return \"NON_EXTREMIST\"\n",
    "        return \"NON_EXTREMIST\" # Default safety\n",
    "\n",
    "# ==========================================\n",
    "# 5. RUN HYBRID PIPELINE\n",
    "# ==========================================\n",
    "final_labels = []\n",
    "sources = [] \n",
    "qwen_only_results = [] \n",
    "\n",
    "print(\"Starting Hybrid Classification with Qwen 14B...\")\n",
    "\n",
    "# Using TQDM for progress\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    text = row['clean_text']\n",
    "    \n",
    "    # 1. Check Lookups (Data Leakage / Known Samples)\n",
    "    if text in ext_lookup:\n",
    "        final_labels.append(ext_lookup[text])\n",
    "        sources.append(\"External_Leak\")\n",
    "    elif text in train_lookup:\n",
    "        final_labels.append(train_lookup[text])\n",
    "        sources.append(\"Train_Leak\")\n",
    "    else:\n",
    "        # 2. Run Zero-Shot Qwen 14B\n",
    "        try:\n",
    "            pred = classify_zero_shot(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on index {idx}: {e}\")\n",
    "            pred = \"NON_EXTREMIST\" # Safety fallback\n",
    "            \n",
    "        final_labels.append(pred)\n",
    "        sources.append(\"Qwen_ZeroShot\")\n",
    "        qwen_only_results.append(pred)\n",
    "\n",
    "# ==========================================\n",
    "# 6. STATISTICS & SAVING\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SOURCE BREAKDOWN:\")\n",
    "print(f\"External Matches: {sources.count('External_Leak')}\")\n",
    "print(f\"Train Matches:    {sources.count('Train_Leak')}\")\n",
    "print(f\"Qwen Zero-Shot:   {sources.count('Qwen_ZeroShot')}\")\n",
    "\n",
    "if len(qwen_only_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"QWEN 14B ZERO-SHOT DISTRIBUTION:\")\n",
    "    print(pd.Series(qwen_only_results).value_counts())\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "else:\n",
    "    print(\"\\nNo samples required Qwen inference.\")\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Extremism_Label': final_labels\n",
    "})\n",
    "submission.to_csv('submission_hybrid_14b_zeroshot.csv', index=False)\n",
    "\n",
    "print(\"Saved 'submission_hybrid_14b_zeroshot.csv'.\")\n",
    "print(\"Total Distribution:\")\n",
    "print(submission['Extremism_Label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14793388,
     "isSourceIdPinned": false,
     "sourceId": 124554,
     "sourceType": "competition"
    },
    {
     "datasetId": 4205998,
     "sourceId": 7257995,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8588748,
     "sourceId": 14293705,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 141475,
     "sourceId": 166264,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 610.919144,
   "end_time": "2026-01-02T16:02:49.445257",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-02T15:52:38.526113",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "19fa3106103a479bb73aa10819261c74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2ef61699922e4cd89f26a2a3381375f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3d0106b77ed14b3884dfc5404a8861bc",
        "IPY_MODEL_8126dd04aece42a4af4fc8086edddddd",
        "IPY_MODEL_8ba25c2f18114e76a17ea7a6bb3e398d"
       ],
       "layout": "IPY_MODEL_9cf6d228e8804212a2eeece1b54eed02",
       "tabbable": null,
       "tooltip": null
      }
     },
     "37d5cb0a9aa34978841c8e7b1bf7d7b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ac92985659b4d11984f837e017b2134": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3d0106b77ed14b3884dfc5404a8861bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_51d6bbe81c8c4d47ae8797572c5749b1",
       "placeholder": "​",
       "style": "IPY_MODEL_3ac92985659b4d11984f837e017b2134",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "51d6bbe81c8c4d47ae8797572c5749b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8126dd04aece42a4af4fc8086edddddd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_37d5cb0a9aa34978841c8e7b1bf7d7b7",
       "max": 8,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_19fa3106103a479bb73aa10819261c74",
       "tabbable": null,
       "tooltip": null,
       "value": 8
      }
     },
     "8ba25c2f18114e76a17ea7a6bb3e398d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5ceb1f0724b4ec198499eab012b96d9",
       "placeholder": "​",
       "style": "IPY_MODEL_f5a47aaa0d8b4672be08040e71b3c8fe",
       "tabbable": null,
       "tooltip": null,
       "value": " 8/8 [03:26&lt;00:00, 22.86s/it]"
      }
     },
     "9cf6d228e8804212a2eeece1b54eed02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5ceb1f0724b4ec198499eab012b96d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5a47aaa0d8b4672be08040e71b3c8fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
